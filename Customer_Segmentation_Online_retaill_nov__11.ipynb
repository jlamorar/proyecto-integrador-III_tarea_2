{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlamorar/proyecto-integrador-III_tarea_2/blob/main/Customer_Segmentation_Online_retaill_nov__11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPyTNRutJTF1"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UVnZ8W-mFCxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Customer Segmentation - Online retail Project**"
      ],
      "metadata": {
        "id": "KmDpbscOFENC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 1: Business Understanding** Cross-Industry Standard Process for Data Mining(CRISP DM)"
      ],
      "metadata": {
        "id": "AQn3t1k8FxiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbc9LRCSJTF2"
      },
      "source": [
        "In ecommerce companies like online retails, customer segmentation is necessary in order to understand customers behaviors. It leverages aqcuired customer data like the one we have in our case, **transactions data** in order to divide customers into groups.\n",
        "\n",
        "Our goal in this Notebook is to cluster our customers to get insights in:\n",
        "- Increasing **revenue** (Knowing customers who present most of our revenue)\n",
        "- Increasing customer **retention**\n",
        "- Discovering **Trends and patterns**\n",
        "- Defining **customers at risk**\n",
        "\n",
        "We will do **RFM Analysis** as a first step and then **combine RFM with predictive algorithms (k-means)**.\n",
        "\n",
        "RFM Analysis answers these questions:\n",
        "- Who are our best customers?\n",
        "- Who has the potential to be converted in more profitable customers?\n",
        "- Which customers we must retain?\n",
        "- Which group of customers is most likely to respond to our current campaign?\n",
        "\n",
        "More about RFM [here](https://www.putler.com/rfm-analysis/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hU7b1BeJTF2"
      },
      "source": [
        "# Importing main libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este bloque importa las bibliotecas necesarias para manipular datos (pandas, numpy), gestionar fechas (datetime), y aplicar modelos de clustering y métricas de evaluación (sklearn).\n",
        "Las bibliotecas de visualización (matplotlib, seaborn) se utilizan para explorar y presentar gráficas que respalden los análisis.\n",
        "La línea %matplotlib inline asegura que las gráficas se rendericen directamente en el notebook, y warnings.filterwarnings(\"ignore\") elimina las advertencias para que el output sea más limpio.\n",
        "Con estas bibliotecas, podemos realizar un análisis completo que incluye la limpieza de datos, el análisis exploratorio y la aplicación de modelos predictivos, respaldado por visualizaciones claras."
      ],
      "metadata": {
        "id": "XSEyrM9n5dzU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "szEX-EXnJTF3"
      },
      "outputs": [],
      "source": [
        "#!pip install -U scikit-learn\n",
        "#!pip install GMM\n",
        "#!pip install shapeGMM\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import time, warnings\n",
        "import datetime as dt\n",
        "\n",
        "#modules for predictive models\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "#from sklearn.mixture import GMM\n",
        "\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "#visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Data Understanding** Cross-Industry Standard Process for Data Mining(CRISP DM)"
      ],
      "metadata": {
        "id": "NoxHubZ1GgyI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axRcJjcsJTF3"
      },
      "source": [
        "# Getting and loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aquí se carga el dataset Online Retail.xlsx en un DataFrame de pandas para analizarlo.\n",
        "head() muestra las primeras 5 filas del dataset, lo que permite obtener una vista preliminar de las columnas y los datos.\n",
        "Vemos las primeras filas del dataset, confirmando que contiene columnas clave como InvoiceNo, CustomerID, Quantity, UnitPrice, entre otras. Esto nos ayudará a identificar las columnas relevantes para el análisis.\n"
      ],
      "metadata": {
        "id": "Lq34TeaG57C8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mC5-hxAJTF3"
      },
      "outputs": [],
      "source": [
        "# Get the dataset from https://www.kaggle.com/datasets/puneetbhaya/online-retail?resource=download\n",
        "#loading the dataset\n",
        "retail_df = pd.read_excel(\"Online Retail.xlsx\")\n",
        "\n",
        "retail_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WUC0JLN8e4-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the Data types"
      ],
      "metadata": {
        "id": "IfsKWJeOe5Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dtypes muestra los tipos de datos de cada columna. Esto es importante para verificar si los datos están en el formato correcto antes de procesarlos.\n",
        "drop(['StockCode'], axis=1) elimina una columna innecesaria que no aporta valor al análisis.\n",
        "Confirmarás que CustomerID es de tipo float64, lo cual es incorrecto, y deberá ser convertido a int. Además, la columna StockCode se eliminará, reduciendo el ruido en el dataset.\n"
      ],
      "metadata": {
        "id": "yu3okdxI63cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retail_df.dtypes\n",
        "#CustomerID is float it must be changed to int datatype\n",
        "#InvoiceNo must be retired from the dataframe"
      ],
      "metadata": {
        "id": "iGkBKAezc9IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminamos valores nulos y convertimos la columna CustomerID a tipo entero"
      ],
      "metadata": {
        "id": "bvtNejy88SDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retail_df = retail_df.dropna(subset=['CustomerID'])\n",
        "retail_df['CustomerID'] = retail_df['CustomerID'].astype(int)\n"
      ],
      "metadata": {
        "id": "CwmQuFOA79zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retail_df.dtypes"
      ],
      "metadata": {
        "id": "yqah4mEb8GqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yEWREA4d79Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dtypes muestra los tipos de datos de cada columna y vemos que CustomerID ahora es de tipo INT. Esto es importante para verificar si los datos están en el formato correcto antes de procesarlos.\n",
        "##drop(['StockCode'], axis=1) elimina una columna innecesaria que no aporta valor al análisis.\n"
      ],
      "metadata": {
        "id": "8UA4kJic8vYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing StockCode\n",
        "retail_df2 = retail_df.drop(['StockCode'], axis=1)\n"
      ],
      "metadata": {
        "id": "9iIsT1SEe-UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verifiying dtypes again\n",
        "retail_df2.dtypes\n"
      ],
      "metadata": {
        "id": "OJlbQI-YgUFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatized Exploratory Data Analysis with ydata_profiling library"
      ],
      "metadata": {
        "id": "FnQcUIfcH6kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este bloque utiliza la biblioteca ydata_profiling para generar un informe exploratorio automático del dataset.El informe incluirá métricas como valores faltantes, duplicados, distribuciones de variables y correlaciones.\n",
        "##Esto generará n informe detallado que resalte problemas potenciales como valores faltantes, columnas con datos altamente correlacionados o distribuciones sesgadas, lo que nos permitirá tomar decisiones informadas en la fase de preprocesamiento.\n"
      ],
      "metadata": {
        "id": "bOj6UHt29jig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling\n"
      ],
      "metadata": {
        "id": "aZFHJrlgv8PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalamos la biblioteca ydata_profiling para automatizar el análisis exploratorio de los datos"
      ],
      "metadata": {
        "id": "2nHLI4YT-3S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##La utilización de esta biblioteca nos sirve para generar un informe exploratorio automático del dataset. El informe incluirá métricas como valores faltantes, duplicados, distribuciones de variables y correlaciones,\n",
        "distribuciones sesgadas. Lo qu nos  permitirá tomar decisiones informadas en la fase de preprocesamiento."
      ],
      "metadata": {
        "id": "vFIaGls7_g0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling\n"
      ],
      "metadata": {
        "id": "yj7yG1l0-h_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importa la clase ProfileReport de la biblioteca ydata_profiling, que se utiliza para generar informes automatizados de análisis exploratorio de datos.\n",
        "#Crea un informe exploratorio basado en el DataFrame retail_df2.\n",
        "Parámetros:\n",
        "retail_df2: Conjunto de datos que será analizado.\n",
        "title='Explorative Analysis Report': Título personalizado para el informe.\n",
        "explorative=True: Activa características avanzadas como la exploración de relaciones complejas entre variables."
      ],
      "metadata": {
        "id": "2D8VLvYQBXth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing and using the library ydata-profiling\n",
        "# Examples https://github.com/ydataai/ydata-profiling\n",
        "#!pip install ydata-profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "profile = ProfileReport(retail_df2, title='Explorative Analisys Report', explorative=True)\n",
        "profile.to_widgets()"
      ],
      "metadata": {
        "id": "obxk5JZAH4qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatized Exploratory Data Analysis with dtale library"
      ],
      "metadata": {
        "id": "DxsDkt3MH8ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalamos la libreria dtale que es una herramienta poderosa para realizar análisis exploratorios de datos de forma interactiva. Una vez instalada correctamente, nos permitirá explorar los datos de manera visual e intuitiva."
      ],
      "metadata": {
        "id": "YfQ63vtoCZUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#D-Tale es una librería nos permite realizar el análisis exploratorio de datos interactivo en un navegador web. Genera visualizaciones, resúmenes estadísticos y análisis interactivos basados en un DataFrame de pandas, facilitando la inspección de los datos sin necesidad de escribir mucho código**"
      ],
      "metadata": {
        "id": "XSnqCOsdDa_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z3d2Ze8mDZrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtale\n"
      ],
      "metadata": {
        "id": "dWrp_WsrCebH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatized Exploratory Data Analysis\n",
        "#installing and using the library dtale\n",
        "#!pip install dtale\n",
        "# library https://pypi.org/project/dtale/\n",
        "import dtale\n",
        "from dtale.views import startup\n",
        "import dtale.app as dtale_app\n",
        "dtale_app.USE_COLAB = True\n",
        "dtale.show(retail_df2)\n",
        "\n",
        "# click on the following to check the Automatized Exploratory Data Analysis Report"
      ],
      "metadata": {
        "id": "CE44ZtCoH9Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTA: El enlace creado:  https://grwzufuf87c-496ff2e9c6d22116-40000-colab.googleusercontent.com/dtale/main/1, es una URL que apunta a un servidor local donde se ejecuta la aplicación interactiva de D-Tale. Esta URL se genera porque estás trabajando en Google Colab, que configura un servidor para ejecutar y mostrar interfaces gráficas."
      ],
      "metadata": {
        "id": "skCnPky6D7Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Data Preparation** Cross-Industry Standard Process for Data Mining(CRISP DM)"
      ],
      "metadata": {
        "id": "AoDyQ11YHuAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Proceso realizado anteriormente pero en este bloque desea verificar los cambios de customerID de float64 a int&$ y la eliminación de la columna StockCode que no es util para el análisis a realizar."
      ],
      "metadata": {
        "id": "5_kMuINrFi4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing null values in CustomerID and converting CustomerID to int datatype with .astype(int) method and\n",
        "retail_df2['CustomerID']  = retail_df2['CustomerID'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "I4ae16eig8wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verifiying CustomerID dtype\n",
        "retail_df2.dtypes"
      ],
      "metadata": {
        "id": "EwpcNT5TyMlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz5FXKozJTF4"
      },
      "source": [
        "# Preparing the Data from the United Kingdom to analyze them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn2dfthJJTF4"
      },
      "source": [
        "As customer clusters may vary by geography, I’ll restrict the data to only United Kingdom customers, which contains most of our customers historical data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filtra los datos para mantener solo registros del Reino Unido"
      ],
      "metadata": {
        "id": "GOdkTl-fGfJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KzH8RGJJTF4"
      },
      "outputs": [],
      "source": [
        "retail_uk = retail_df2[retail_df['Country']=='United Kingdom']\n",
        "#checking the shape\n",
        "# customers in United Kingdom 495478\n",
        "retail_uk.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Este filtro selecciona solo las filas donde la columna Quantity (cantidad de productos en la orden) es mayor que 0.En este contexto, una Quantity menor o igual a 0 indica:\n",
        "##Órdenes canceladas (devueltas).\n",
        "##Errores en la facturación.\n",
        "##Al filtrar por Quantity > 0, se asegura que solo se analicen transacciones válidas.\n",
        "##retail_uk.shape:Devuelve la forma del DataFrame después de aplicar el filtro, es decir, el número de filas y columnas restantes."
      ],
      "metadata": {
        "id": "xGrOiHbvHBi4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prKFRbEhJTF5"
      },
      "outputs": [],
      "source": [
        "#removing canceled orders from the United Kingdom\n",
        "retail_uk = retail_uk[retail_uk['Quantity']>0]\n",
        "retail_uk.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este bloque elimina filas donde el campo CustomerID es nulo (NA) en el DataFrame retail_uk. La presencia de valores nulos en esta columna indica transacciones que no tienen un cliente asociado, lo que puede ser irrelevante para ciertos análisis, como la segmentación de clientes."
      ],
      "metadata": {
        "id": "sj4cPLtoI_4g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZlLzj2tJTF5"
      },
      "outputs": [],
      "source": [
        "#removing instances where customerID is NA\n",
        "retail_uk.dropna(subset=['CustomerID'],how='all',inplace=True)\n",
        "retail_uk.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este bloque filtra el DataFrame retail_uk para restringir los datos a un periodo de tiempo específico, en este caso, a transacciones realizadas a partir del 9 de diciembre de 2010. Esto es fundamental en el análisis RFM (Recency, Frequency, Monetary), ya que es más efectivo analizar métricas basadas en períodos definidos como meses o años.\n",
        "\n"
      ],
      "metadata": {
        "id": "-mBrNGBTKus1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip5Oqb-QJTF5"
      },
      "outputs": [],
      "source": [
        "#restrict the data to one full year because it's better to use a metric per Months or Years in RFM Analysis\n",
        "retail_uk = retail_uk[retail_uk['InvoiceDate']>= \"2010-12-09\"]\n",
        "retail_uk.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Este bloque tiene como objetivo proporcionar un resumen básico del dataset retail_uk después de las etapas de limpieza y filtrado. Específicamente, analiza la cantidad de transacciones, clientes únicos y el porcentaje de valores nulos en CustomerID."
      ],
      "metadata": {
        "id": "nGmQUl8OLw0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srtIXRc-JTF5"
      },
      "outputs": [],
      "source": [
        "print(\"Summary..\")\n",
        "#exploring the unique values of each attribute\n",
        "print(\"Number of transactions: \", retail_uk['InvoiceNo'].nunique())\n",
        "print(\"Number of customers:\", retail_uk['CustomerID'].nunique() )\n",
        "print(\"Percentage of customers NA: \", round(retail_uk['CustomerID'].isnull().sum() * 100 / len(retail_df),2),\"%\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "Hz5NH1JFQdgt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cytkUtbPJTF5"
      },
      "source": [
        "# Important things about a Recency, Frecuency and Monetary Value Analysis (RFM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v4M9KY7JTF5"
      },
      "source": [
        "RFM (**Recency, Frequency, Monetary**) analysis is a customer segmentation technique that uses past purchase **behavior** to divide customers into groups. <br> RFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.\n",
        "- RECENCY (R): Days since last purchase\n",
        "- FREQUENCY (F): Total number of purchases\n",
        "- MONETARY VALUE (M): Total money this customer spent.\n",
        "\n",
        "We will create those 3 customer attributes for each customer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VoZK2xAlQM_9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv6dbSFZJTF5"
      },
      "source": [
        "## Obtaining the Recency (how many days ago was the customer's last purchase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFoh4cMxJTF5"
      },
      "source": [
        "To calculate recency, we need to choose a date point from which we evaluate **how many days ago was the customer's last purchase**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wla3u_QJTF6"
      },
      "outputs": [],
      "source": [
        "#last date available in our dataset\n",
        "retail_uk['InvoiceDate'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "871Ge6LiJTF6"
      },
      "source": [
        "The last date we have is 2011-12-09 so we will use it as reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy53hoOEJTF6"
      },
      "outputs": [],
      "source": [
        "# Stablishing the date in python to do the RFM analysis\n",
        "now = dt.date(2011,12,9)\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-ClE4dkKJTF6"
      },
      "outputs": [],
      "source": [
        "#create a new column called date which contains the date of invoice only\n",
        "retail_uk['date'] = retail_uk['InvoiceDate'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP4js3C5JTF6"
      },
      "outputs": [],
      "source": [
        "retail_uk.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ECVfPW6JTF6"
      },
      "outputs": [],
      "source": [
        "#group by customers and check last date of purchase\n",
        "recency_df = retail_uk.groupby(by='CustomerID', as_index=False)['date'].max()\n",
        "recency_df.columns = ['CustomerID','LastPurchaseDate']\n",
        "recency_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b_P4qUTZJTF6"
      },
      "outputs": [],
      "source": [
        "#creating variable Recency and calculating recency in days\n",
        "recency_df['Recency'] = recency_df['LastPurchaseDate'].apply(lambda x: (now - x).days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijRoIZ1qJTF6"
      },
      "outputs": [],
      "source": [
        "#checking recency_df content\n",
        "recency_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zS9en4xnJTF7"
      },
      "outputs": [],
      "source": [
        "#drop LastPurchaseDate as we don't need it anymore\n",
        "recency_df.drop('LastPurchaseDate',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking CustomerID and Recency\n",
        "recency_df.head()"
      ],
      "metadata": {
        "id": "v-wqNJtgOhcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Instances and variables in recency_df.shape\n",
        "recency_df.shape"
      ],
      "metadata": {
        "id": "FTHNPMYuPCeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF-oKV8VJTF7"
      },
      "source": [
        "Now we have the recency attribute created. e.g: Customer with ID = 12346 did his/her last purshace 325 days ago."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El código presentado en el bloque pasado tiene como objetivo calcular la recencia de los clientes, es decir, el número de días transcurridos desde su última compra, con base en un conjunto de datos de facturas. Primero, se define una fecha de referencia (now), que es el 9 de diciembre de 2011, para calcular la diferencia en días. Luego, se extrae únicamente la fecha de las facturas (date) y se agrupan los datos por CustomerID para encontrar la última fecha de compra de cada cliente (LastPurchaseDate). Posteriormente, se calcula la recencia creando una nueva columna Recency, que mide la diferencia en días entre la fecha de referencia y la última compra. Una vez obtenido este dato, se elimina la columna de LastPurchaseDate para simplificar el DataFrame y se verifica la estructura final del mismo, que contiene únicamente el ID del cliente y su recencia. Este análisis es fundamental en estrategias de marketing para identificar clientes recientes o inactivos y desarrollar campañas personalizadas basadas en su comportamiento de compra."
      ],
      "metadata": {
        "id": "Y6YY-tUiVsZU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b56fn2eJTF7"
      },
      "source": [
        "# Obtaining the Frequency (how many times a customer purchased from the Online Retail Company)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b9A56NBJTF7"
      },
      "source": [
        "Frequency helps us to know **how many times a customer purchased from us**. To do that we need to check how many invoices are registered by the same customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW5L-flBJTF7"
      },
      "outputs": [],
      "source": [
        "# drop duplicates CustomerIDs\n",
        "retail_uk_copy = retail_uk\n",
        "retail_uk_copy.drop_duplicates(subset=['InvoiceNo', 'CustomerID'], keep=\"first\", inplace=True)\n",
        "\n",
        "\n",
        "#Calculate frequency of purchases by CustomerID\n",
        "frequency_df = retail_uk_copy.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\n",
        "frequency_df.columns = ['CustomerID','Frequency']\n",
        "frequency_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este bloque de codigo tiene como finalidad calcular la frecuencia de compras de los clientes, es decir, cuántas veces un cliente ha realizado una transacción en el conjunto de datos. Primero, se realiza una copia del DataFrame original retail_uk para trabajar sobre ella sin modificar el original. Luego, se eliminan las filas duplicadas en la combinación de las columnas InvoiceNo (número de factura) y CustomerID, manteniendo solo la primera ocurrencia. Esto asegura que cada factura de un cliente se cuente solo una vez. Posteriormente, se agrupan los datos por CustomerID y se cuenta la cantidad de facturas (InvoiceNo) asociadas a cada cliente. El resultado se almacena en un nuevo DataFrame frequency_df, que contiene dos columnas: CustomerID y Frequency, donde esta última indica cuántas transacciones ha realizado cada cliente. Este análisis es útil para medir la frecuencia de compra, un componente clave del modelo RFM (Recency, Frequency, Monetary) utilizado en la segmentación de clientes y la toma de decisiones estratégicas en marketing."
      ],
      "metadata": {
        "id": "gqAPsWGWWz3n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS2hpWx8JTF7"
      },
      "source": [
        "## Monetary Value (How much money a customer spent over time in the Online Retail Company)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA4VpS2cJTF7"
      },
      "source": [
        "Monetary attribute answers the question: **How much money did the customer spent over time?**\n",
        "\n",
        "To do that, first, we will create a new column total cost to have the total price per invoice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "leuExAmRJTF7"
      },
      "outputs": [],
      "source": [
        "#create column TotalCost from customers in United Kingdom\n",
        "retail_uk['TotalCost'] = retail_uk['Quantity'] * retail_uk['UnitPrice']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgksnvurJTF7"
      },
      "outputs": [],
      "source": [
        "# Obtaining the sum or TotalCost of all purchases by CustomerID\n",
        "monetary_df = retail_uk.groupby(by='CustomerID',as_index=False).agg({'TotalCost': 'sum'})\n",
        "monetary_df.columns = ['CustomerID','Monetary']\n",
        "monetary_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código tiene como propósito calcular el valor monetario total de las compras realizadas por cada cliente en el Reino Unido, un componente fundamental del análisis RFM.\n",
        "\n",
        "Primero, se crea una nueva columna en el DataFrame retail_uk llamada TotalCost, que se obtiene multiplicando la cantidad de productos comprados (Quantity) por el precio unitario (UnitPrice) de cada transacción. Esto representa el costo total de cada factura.\n",
        "\n",
        "Luego, se agrupan los datos por CustomerID utilizando la función groupby y se calcula la suma total de los valores de TotalCost para cada cliente mediante la función de agregación agg. El resultado se guarda en un nuevo DataFrame monetary_df, que contiene dos columnas: CustomerID y Monetary. La columna Monetary refleja el valor monetario total gastado por cada cliente. Finalmente, se utiliza head() para mostrar las primeras cinco filas de este DataFrame, facilitando la revisión de los resultados.\n",
        "\n",
        "Este análisis es crucial para entender el valor económico que cada cliente aporta al negocio, permitiendo segmentar a los clientes según su valor y priorizar estrategias de retención o fidelización para los más rentables."
      ],
      "metadata": {
        "id": "O-aMJa-iYTpR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qN_TL9JTF7"
      },
      "source": [
        "# Creating the Recency, Frequency and Monetary value dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDIUd59mJTF8"
      },
      "outputs": [],
      "source": [
        "#merge recency dataframe with frequency dataframe\n",
        "temp_df = recency_df.merge(frequency_df,on='CustomerID')\n",
        "temp_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Ny3Q6XJTF8"
      },
      "outputs": [],
      "source": [
        "#merging the monetary_df dataframe temp_df\n",
        "rfm_df = temp_df.merge(monetary_df,on='CustomerID')\n",
        "\n",
        "#and using CustomerID as index in the dataframe\n",
        "rfm_df.set_index('CustomerID',inplace=True)\n",
        "\n",
        "#check the head\n",
        "rfm_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Código anterior combina los DataFrames de recencia, frecuencia y valor monetario total de las compras para construir un único DataFrame consolidado llamado rfm_df, utilizado en el análisis RFM (Recency, Frequency, Monetary). Primero, se fusionan los DataFrames de recencia (recency_df) y frecuencia (frequency_df) utilizando la columna CustomerID como clave, creando un DataFrame temporal (temp_df) que incluye las columnas Recency y Frequency. Luego, este se fusiona con el DataFrame de valor monetario (monetary_df) para incorporar la columna Monetary. Finalmente, se establece CustomerID como índice del DataFrame resultante, facilitando el acceso a los datos de cada cliente. El DataFrame final rfm_df permite analizar de manera integral el comportamiento de compra de los clientes, proporcionando métricas clave para segmentación y estrategias de marketing."
      ],
      "metadata": {
        "id": "lDHeYv_7bIEQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhecvweJTF8"
      },
      "source": [
        "Customer with ID = 12346 has recency: 325 days, frequency:1, and monetary: 77183,60 £."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clu8zBdIJTF8"
      },
      "source": [
        "### RFM Table Correctness verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nEpj-wNJTF8"
      },
      "outputs": [],
      "source": [
        "retail_uk[retail_uk['CustomerID']==12346.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPVEOW7UJTF9"
      },
      "outputs": [],
      "source": [
        "(now - dt.date(2011,1,18)).days == 325"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Estas dos lineas de codigo filtran las transacciones del cliente con CustomerID 12346.0 en el DataFrame retail_uk, mostrando todas las compras realizadas por este cliente, como fechas, cantidades y precios, para analizar su comportamiento. Posteriormente, calcula la diferencia en días entre el 9 de diciembre de 2011 (now) y el 18 de enero de 2011 (dt.date(2011,1,18)), lo que da un total de 325 días, y verifica si este valor coincide con el esperado utilizando la expresión lógica == 325. Este análisis es útil para validar cálculos de recencia en el contexto del modelo RFM y asegurar que los datos sean precisos al medir el tiempo desde la última compra de un cliente."
      ],
      "metadata": {
        "id": "eEA4sfIZcpYl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gofz6D-JTF9"
      },
      "source": [
        "As we can see our RFM table is correct. The first customer bought only once, and only one product with huge amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1peHJW7lJTF9"
      },
      "source": [
        "## Customer segments with RFM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPbnUeizJTF9"
      },
      "source": [
        "Before moving to customer segments, Let's see the application of Pareto Principle – commonly referred to as the 80-20 rule on our dataset by applying it to our RFM variables.\n",
        "\n",
        "Pareto’s rule says **80% of the results come from 20% of the causes**.\n",
        "\n",
        "Similarly, **20% customers contribute to 80% of your total revenue**. Let's verify that because that will help us know which customers to focus on when marketing new products."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Verificando la ley de PARETO"
      ],
      "metadata": {
        "id": "3aOcJ_uZdJzG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHZzFyxsJTF9"
      },
      "source": [
        "### Applying 80-20 rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2aEa2NWJTF9"
      },
      "outputs": [],
      "source": [
        "#get the 80% of the revenue\n",
        "pareto_cutoff = rfm_df['Monetary'].sum() * 0.8\n",
        "print(\"The 80% of total revenue is: \",round(pareto_cutoff,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ALAHgFJTF9"
      },
      "outputs": [],
      "source": [
        "customers_rank = rfm_df\n",
        "# Create a new column that is the rank of the value of coverage in ascending order\n",
        "customers_rank['Rank'] = customers_rank['Monetary'].rank(ascending=0)\n",
        "#customers_rank.drop('RevenueRank',axis=1,inplace=True)\n",
        "customers_rank.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El bloque anterior implementa un análisis basado en el principio de Pareto (80/20) para identificar a los clientes más valiosos que generan el 80% de los ingresos totales. Primero, calcula el 80% del ingreso total sumando los valores de la columna Monetary en el DataFrame rfm_df y multiplicando por 0.8, almacenando este valor en la variable pareto_cutoff. Luego, se agrega una nueva columna Rank al DataFrame customers_rank, que clasifica a los clientes según su contribución monetaria de mayor a menor (ascending=0). Este ranking facilita identificar rápidamente cuáles clientes aportan la mayor parte de los ingresos, lo que es fundamental para la segmentación y estrategias de fidelización enfocadas en los clientes más rentables."
      ],
      "metadata": {
        "id": "-tKKCjsGdwRN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZRZAp1bJTF9"
      },
      "source": [
        "### Top Customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1b_xqWFJTF-"
      },
      "outputs": [],
      "source": [
        "customers_rank.sort_values('Rank',ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc_rZmK6JTF-"
      },
      "outputs": [],
      "source": [
        "#get top 20% of the customers\n",
        "top_20_cutoff = 3863 *20 /100\n",
        "top_20_cutoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQRaeSehJTF-"
      },
      "outputs": [],
      "source": [
        "#sum the monetary values over the customer with rank <=773\n",
        "revenueByTop20 = customers_rank[customers_rank['Rank'] <= 772]['Monetary'].sum()\n",
        "revenueByTop20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#En este bloque anterior se ordena el DataFrame customers_rank por la columna Rank en orden ascendente para priorizar a los clientes con mayor contribución monetaria. Luego, calcula el número de clientes que representan el 20% superior del total, determinando que son 772 clientes de un total de 3863. Posteriormente, filtra estos clientes utilizando la columna Rank y suma sus valores monetarios (Monetary) para obtener el ingreso total generado por el top 20%, almacenándolo en la variable revenueByTop20. Este análisis confirma que una pequeña fracción de los clientes (20%) genera la mayor parte de los ingresos, alineándose con el principio de Pareto, y permite a la empresa enfocar sus esfuerzos en estos clientes clave para maximizar la rentabilidad."
      ],
      "metadata": {
        "id": "E9L1UnlHemYR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPwaA04gJTF-"
      },
      "source": [
        "In our case, the 80% of total revenue is not achieved by the 20% of TOP customers but approximately, it does, because they are less than our 20% TOP customers who achieve it. It would be interesting to study this group of customers because they are those who make our most revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4TjMiWJTF-"
      },
      "source": [
        "### Applying RFM score formula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOaIx2LhJTF-"
      },
      "source": [
        "The simplest way to create customers segments from RFM Model is to use **Quartiles**. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best/highest value, and one is the lowest/worst value. A final RFM score is calculated simply by combining individual RFM score numbers.\n",
        "\n",
        "Note: Quintiles (score from 1-5) offer better granularity, in case the business needs that but it will be more challenging to create segments since we will have 5*5*5 possible combinations. So, we will use quartiles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEhO3olJTF-"
      },
      "source": [
        "#### RFM Quartiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6JLNknIJTF-"
      },
      "outputs": [],
      "source": [
        "quantiles = rfm_df.quantile(q=[0.25,0.5,0.75])\n",
        "quantiles"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DvZKlQXKfd1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWfxYP71JTF_"
      },
      "outputs": [],
      "source": [
        "quantiles.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El código calcula los cuartiles (25%, 50% y 75%) de las métricas RFM (Recency, Frequency y Monetary) en el DataFrame rfm_df mediante la función quantile, lo que permite dividir los datos en cuatro partes iguales y entender la distribución de cada métrica. Luego, convierte estos cuartiles en un diccionario utilizando to_dict(), lo que facilita su uso posterior para segmentar clientes en diferentes categorías según su comportamiento de compra. Esta segmentación es clave para identificar patrones y desarrollar estrategias personalizadas para distintos grupos de clientes."
      ],
      "metadata": {
        "id": "TWBKk3qxfWdE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91ab2RNJTF_"
      },
      "source": [
        "#### Creation of RFM segmentation table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy_2RvLdJTF_"
      },
      "source": [
        "We will create two segmentation classes since, high recency is bad, while high frequency and monetary value is good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o_2pOwzaJTF_"
      },
      "outputs": [],
      "source": [
        "# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\n",
        "def RScore(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 4\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 3\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)\n",
        "def FMScore(x,p,d):\n",
        "    if x <= d[p][0.25]:\n",
        "        return 1\n",
        "    elif x <= d[p][0.50]:\n",
        "        return 2\n",
        "    elif x <= d[p][0.75]:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NOtx3DCpJTF_"
      },
      "outputs": [],
      "source": [
        "#create rfm segmentation table\n",
        "rfm_segmentation = rfm_df\n",
        "rfm_segmentation['R_Quartile'] = rfm_segmentation['Recency'].apply(RScore, args=('Recency',quantiles,))\n",
        "rfm_segmentation['F_Quartile'] = rfm_segmentation['Frequency'].apply(FMScore, args=('Frequency',quantiles,))\n",
        "rfm_segmentation['M_Quartile'] = rfm_segmentation['Monetary'].apply(FMScore, args=('Monetary',quantiles,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl6TUcv5JTF_"
      },
      "outputs": [],
      "source": [
        "rfm_segmentation.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Los bloques anteriores implementan un sistema de puntuación basado en los cuartiles de las métricas RFM (Recency, Frequency y Monetary) para segmentar a los clientes. Primero, define dos funciones: RScore y FMScore. La función RScore asigna puntuaciones inversas a la recencia, donde los clientes con valores más bajos (reciente) obtienen una puntuación más alta (4 para el mejor cuartil). Por otro lado, la función FMScore asigna puntuaciones directas a la frecuencia y el valor monetario, donde los clientes con valores más altos obtienen mejores puntuaciones (4 para el mejor cuartil). Luego, se crea un nuevo DataFrame rfm_segmentation, al que se añaden tres nuevas columnas (R_Quartile, F_Quartile y M_Quartile) que contienen las puntuaciones basadas en la posición de cada cliente dentro de los cuartiles de cada métrica. Este sistema de puntuación permite clasificar a los clientes en segmentos clave según su comportamiento de compra y facilita la toma de decisiones estratégicas en marketing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ooL8iZehDaL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEW8HygJTF_"
      },
      "source": [
        "Now that we have the score of each customer, we can represent our customer segmentation.<br>\n",
        "First, we need to combine the scores (R_Quartile, F_Quartile,M_Quartile) together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpYJIawQJTF_"
      },
      "outputs": [],
      "source": [
        "rfm_segmentation['RFMScore'] = rfm_segmentation.R_Quartile.map(str) \\\n",
        "                            + rfm_segmentation.F_Quartile.map(str) \\\n",
        "                            + rfm_segmentation.M_Quartile.map(str)\n",
        "rfm_segmentation.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El código crea una nueva columna llamada RFMScore en el DataFrame rfm_segmentation, que combina las puntuaciones individuales de Recency, Frequency y Monetary en un único valor concatenado. Utiliza map(str) para convertir las puntuaciones de cada métrica en cadenas de texto y luego las concatena, generando un puntaje como \"423\", donde cada dígito representa la posición del cliente en los cuartiles de las métricas correspondientes. Este puntaje resume el comportamiento del cliente en un solo indicador, facilitando su segmentación en grupos específicos según su valor para la empresa. Finalmente, se utiliza head() para verificar las primeras filas del DataFrame y confirmar que la columna se haya generado correctamente, lo que permite identificar clientes clave para estrategias de marketing personalizadas."
      ],
      "metadata": {
        "id": "laMFdNGfikp2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMBp0xj-JTF_"
      },
      "source": [
        "Best Recency score = 4: most recently purchase.\n",
        "Best Frequency score = 4: most quantity purchase.\n",
        "Best Monetary score = 4: spent the most.\n",
        "\n",
        "Let's see who are our **Champions** (best customers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxc0ImnOJTGA"
      },
      "outputs": [],
      "source": [
        "rfm_segmentation[rfm_segmentation['RFMScore']=='444'].sort_values('Monetary', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código filtra el DataFrame rfm_segmentation para identificar a los clientes con la mejor puntuación posible de RFMScore, es decir, '444'. Esto significa que estos clientes se encuentran en el mejor cuartil para Recency (recientes), Frequency (frecuentes) y Monetary (mayor gasto). Luego, los resultados se ordenan por la columna Monetary en orden descendente, mostrando primero a los clientes que han gastado más. Finalmente, se utiliza head(10) para visualizar los 10 clientes con mayor valor monetario dentro del segmento más valioso, lo que permite identificar a los clientes más importantes en términos de ingresos y actividad reciente. Esto es clave para diseñar estrategias de fidelización enfocadas en mantener la relación con estos clientes."
      ],
      "metadata": {
        "id": "NMsPGhLtjDqI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7sCZKNyJTGA"
      },
      "source": [
        "We can find [here](http://www.blastam.com/blog/rfm-analysis-boosts-sales) a suggestion of key segments and then we can decide which segment to consider for further study.\n",
        "\n",
        "**Note:** the suggested link use the opposite valuation: 1 as highest/best score and 4 is the lowest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y__DA4hJTGA"
      },
      "source": [
        "**How many customers do we have in each segment?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AHDMmtgJTGA"
      },
      "outputs": [],
      "source": [
        "print(\"Best Customers: \",len(rfm_segmentation[rfm_segmentation['RFMScore']=='444']))\n",
        "print('Loyal Customers: ',len(rfm_segmentation[rfm_segmentation['F_Quartile']==4]))\n",
        "print(\"Big Spenders: \",len(rfm_segmentation[rfm_segmentation['M_Quartile']==4]))\n",
        "print('Almost Lost: ', len(rfm_segmentation[rfm_segmentation['RFMScore']=='244']))\n",
        "print('Lost Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='144']))\n",
        "print('Lost Cheap Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='111']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código clasifica a los clientes en categorías clave según sus puntuaciones RFM y cuenta cuántos pertenecen a cada segmento. Los Best Customers (puntuación '444') son los más valiosos por su alta recencia, frecuencia y valor monetario. Los Loyal Customers (frecuencia alta) compran regularmente, mientras que los Big Spenders (valor monetario alto) generan mayores ingresos. Los Almost Lost ('244') eran clientes frecuentes y de alto valor, pero su actividad reciente ha disminuido. Los Lost Customers ('144') gastaban mucho, pero no han comprado recientemente, y los Lost Cheap Customers ('111') son los menos valiosos, con poca recencia, baja frecuencia y bajo gasto. Esta segmentación permite estrategias personalizadas para cada grupo."
      ],
      "metadata": {
        "id": "6uTCIgBOkoXT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "y1kPEwF7JTGA"
      },
      "source": [
        "Now that we knew our customers segments we can choose how to target or deal with each segment.\n",
        "\n",
        "For example:\n",
        "\n",
        "**Best Customers - Champions**: Reward them. They can be early adopters to new products. Suggest them \"Refer a friend\".\n",
        "\n",
        "**At Risk**: Send them personalized emails to encourage them to shop.\n",
        "\n",
        "More ideas about what actions to perform in [Ometria](http://54.73.114.30/customer-segmentation#)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLrhcueJTGA"
      },
      "source": [
        "### Conclusion - perspective from this level of customer segmentation\n",
        "To gain even further insight into customer behavior, we can dig deeper in the relationship between RFM variables.  \n",
        "\n",
        "RFM model can be used in conjunction with certain predictive models like **k-means clustering**, **Logistic Regression** and **Recommendation** to produce better informative results on customer behavior.\n",
        "\n",
        "We will go for k-means since it has been widely used for Market Segmentation and it offers the advantage of being simple to implement, following Andrew Ng who advice in his Machine Learning course, start with a dirty and simple model then move to more complex models because simple implementation helps having a first glance at the data and know where/how to exploit it better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JavleEadJTGA"
      },
      "source": [
        "## Applying K-means clustering on RFM variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZN_KBTEJTGA"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4vld1XKJTGB"
      },
      "outputs": [],
      "source": [
        "rfm_data = rfm_df.drop(['R_Quartile','F_Quartile','M_Quartile','RFMScore'],axis=1)\n",
        "rfm_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código elimina las columnas R_Quartile, F_Quartile, M_Quartile y RFMScore del DataFrame rfm_df, dejando únicamente las métricas originales de Recency, Frequency y Monetary en un nuevo DataFrame llamado rfm_data. Esto se hace para enfocarse en las variables numéricas sin incluir las clasificaciones derivadas, permitiendo un análisis más limpio y adecuado para técnicas como clustering o modelos estadísticos. Finalmente, se utiliza head() para verificar que el DataFrame resultante contenga solo las columnas requeridas."
      ],
      "metadata": {
        "id": "hyS1pe_Xlj6h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzgjcoBFJTGB"
      },
      "source": [
        "#### Feature correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Dkt5dDJTGB"
      },
      "outputs": [],
      "source": [
        "rfm_data.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG0SjcbhJTGB"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(rfm_data.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Estas 2 líneas de codigo calculan la matriz de correlación de las métricas Recency, Frequency y Monetary en el DataFrame rfm_data utilizando corr(), lo que permite identificar la relación lineal entre estas variables. Posteriormente, se visualiza esta matriz con un mapa de calor (heatmap) de la librería seaborn, donde los colores indican la magnitud y dirección de las correlaciones. Un valor cercano a 1 o -1 muestra una fuerte correlación positiva o negativa, respectivamente, mientras que un valor cercano a 0 indica poca o ninguna relación. Este análisis ayuda a entender cómo estas métricas están relacionadas, lo que puede ser clave para ajustar estrategias de segmentación o validar la independencia de las variables para modelos predictivos."
      ],
      "metadata": {
        "id": "3Rl-jB2UmNNY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfCdnZglJTGB"
      },
      "source": [
        "On one hand, we have a negative correlation between:\n",
        "- Recency and Frequency\n",
        "- Recency and Monetary\n",
        "\n",
        "On the other hand, the correlation between **Monetary and Frequency** is positive comparing to negative ones but still not that strong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-_A44tEJTGB"
      },
      "source": [
        "#### Visualize feature distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_3Q7eXvJTGB"
      },
      "source": [
        "To get a better understanding of the dataset, we can construct a scatter matrix of each of the three features present in the RFM data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIR_nls-JTGB"
      },
      "outputs": [],
      "source": [
        "# Produce a scatter matrix for each pair of features in the data\n",
        "scatter_matrix(rfm_data, alpha = 0.3, figsize = (11,5), diagonal = 'kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este codigo genera una matriz de dispersión para las métricas Recency, Frequency y Monetary en el DataFrame rfm_data utilizando la función scatter_matrix. Esta matriz muestra gráficos de dispersión para cada par de variables, permitiendo visualizar la relación entre ellas. Además, en la diagonal principal se presentan gráficos de densidad (especificados por diagonal='kde'), que muestran la distribución de cada métrica individualmente. La transparencia de los puntos se controla con el parámetro alpha=0.3, y el tamaño del gráfico se ajusta con figsize=(11,5). Este análisis visual es útil para identificar patrones, correlaciones no lineales y posibles outliers en las métricas RFM, facilitando la interpretación del comportamiento de los clientes"
      ],
      "metadata": {
        "id": "tbQzknU1nFZQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFrmKvGCJTGB"
      },
      "source": [
        "We can notice that we have a **skewed distribution** of the 3 variables and there exist **outliers**.\n",
        "\n",
        "This indicates how normalization is required to make the data features normally distributed as **clustering** algorithms **require** them to be **normally distributed**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD92WrF7JTGC"
      },
      "source": [
        "#### Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S3J-BVNRJTGC"
      },
      "outputs": [],
      "source": [
        "#log transformation\n",
        "rfm_r_log = np.log(rfm_data['Recency']+0.1) #can't take log(0) and so add a small number\n",
        "rfm_f_log = np.log(rfm_data['Frequency'])\n",
        "rfm_m_log = np.log(rfm_data['Monetary']+0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MFISptXBJTGC"
      },
      "outputs": [],
      "source": [
        "log_data = pd.DataFrame({'Monetary': rfm_m_log,'Recency': rfm_r_log,'Frequency': rfm_f_log})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBI6vJnjJTGC"
      },
      "outputs": [],
      "source": [
        "log_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El código aplica una transformación logarítmica a las métricas Recency, Frequency y Monetary para reducir la dispersión y manejar mejor las distribuciones sesgadas. Dado que el logaritmo de cero no está definido, se suma un pequeño valor (0.1) a las columnas Recency y Monetary antes de aplicar la transformación. Esto ayuda a estabilizar la varianza y a hacer que los datos sean más adecuados para análisis estadísticos o modelos de machine learning que asumen distribuciones normales. Finalmente, se crea un nuevo DataFrame log_data con las métricas transformadas y se utiliza head() para verificar los primeros registros, lo que permite observar el efecto de la transformación en los valores."
      ],
      "metadata": {
        "id": "NnoCMgx8oKo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQrGHDnJTGC"
      },
      "outputs": [],
      "source": [
        "# Produce a scatter matrix for each pair of features in the data\n",
        "scatter_matrix(log_data, alpha = 0.2, figsize = (11,5), diagonal = 'kde');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código genera una matriz de dispersión para las métricas transformadas logarítmicamente (Recency, Frequency y Monetary) contenidas en el DataFrame log_data. Este gráfico muestra relaciones visuales entre cada par de variables, permitiendo observar si la transformación ha logrado una distribución más uniforme y lineal de los datos. En la diagonal principal, se presentan gráficos de densidad (kde), que ilustran cómo se distribuyen las métricas individuales después de la transformación. La transparencia de los puntos se ajusta con alpha=0.2 para facilitar la visualización en áreas densas, y el tamaño general del gráfico se define con figsize=(11,5). Este análisis ayuda a identificar patrones más claros y mejora la interpretación de las relaciones entre las métricas transformadas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "56hqiQUOowA_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHPCWaUJTGC"
      },
      "source": [
        "The distributions of Frequency and Monetary are better, more normalized, but it's not the case with Recency Distribution, which is improved but not as much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3hZWKPOJTGC"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(rfm_data.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNYqvdJhJTGC"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(log_data.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gFn536bJTGD"
      },
      "outputs": [],
      "source": [
        "log_data.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Los 3 bloques anteriores calculan y visualizan las matrices de correlación tanto para las métricas originales (rfm_data) como para las métricas transformadas logarítmicamente (log_data) utilizando mapas de calor (heatmap) y la función corr(). Estas matrices muestran la relación lineal entre las variables Recency, Frequency y Monetary, con valores entre -1 y 1 que indican la dirección y magnitud de las correlaciones. Al comparar las matrices, se puede observar cómo la transformación logarítmica afecta las correlaciones, pudiendo suavizar relaciones no lineales y mejorar la interpretabilidad. Los mapas de calor ofrecen una representación visual clara, donde colores más oscuros indican correlaciones más fuertes. Este análisis es útil para validar la idoneidad de las transformaciones y su impacto en la relación entre las métricas."
      ],
      "metadata": {
        "id": "RLZYm9zDpyFf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARZxhCcGJTGD"
      },
      "source": [
        "Now, Monetary and Frequency are more strongly correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7Wp_HWJTGD"
      },
      "source": [
        "### K-means Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUK3F2OEJTGD"
      },
      "source": [
        "A common challenge with k-means is that you must tell it how many clusters you expect. Figuring out how many clusters we need is not obvious from data, thus we will try different clusters numbers and check their [silhouette coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). The [elbow](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_elbow_method) method can be used to determine the number of clusters as well.\n",
        "\n",
        "**Note:** K-means is sensitive to initializations because those initializations are critical to quality of optima found. Thus, we will use smart initialization called ***k-means++***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoBMzQApJTGD"
      },
      "outputs": [],
      "source": [
        "matrix = log_data.as_matrix()\n",
        "for n_clusters in range(2,10):\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=100)\n",
        "    kmeans.fit(matrix)\n",
        "    clusters = kmeans.predict(matrix)\n",
        "    silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Este código utiliza el algoritmo K-Means para segmentar clientes en el conjunto de datos transformados log_data, evaluando la calidad de la segmentación mediante el silhouette score. Convierte el DataFrame en una matriz para su procesamiento y realiza iteraciones con diferentes números de clusters, desde 2 hasta 9. En cada iteración, entrena el modelo de K-Means con inicialización k-means++ y ejecuta 100 iteraciones para seleccionar el mejor resultado. Luego, predice los clusters y calcula el silhouette score, que mide la cohesión interna y separación entre clusters. Finalmente, imprime el promedio del silhouette score para cada número de clusters, ayudando a identificar el número óptimo de grupos que maximiza la calidad de la segmentación."
      ],
      "metadata": {
        "id": "wnYzpySBqilX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrpUslZaJTGD"
      },
      "source": [
        "The **best silhouette score** obtained is when the **number of clusters is 2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsaxMA9CJTGD"
      },
      "outputs": [],
      "source": [
        "n_clusters = 2\n",
        "kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
        "kmeans.fit(matrix)\n",
        "clusters_customers = kmeans.predict(matrix)\n",
        "silhouette_avg = silhouette_score(matrix, clusters_customers)\n",
        "print('score de silhouette: {:<.3f}'.format(silhouette_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aquí se realiza una segmentación de clientes utilizando el algoritmo K-Means con 2 clusters y evalúa la calidad de la segmentación a través del silhouette score. Se utiliza la inicialización k-means++ para optimizar la elección de centroides y se ejecuta el algoritmo 30 veces (n_init=30) para garantizar un resultado robusto. Después de ajustar el modelo a los datos, se predicen los clusters de cada cliente en la matriz transformada y se calcula el silhouette score, que mide la cohesión y separación de los clusters. Finalmente, se imprime el score con tres decimales, proporcionando una métrica que permite evaluar qué tan bien están agrupados los clientes en estos 2 clusters."
      ],
      "metadata": {
        "id": "A7KPlxI4rWcP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8GLVDxtJTGD"
      },
      "source": [
        "#### Visualize Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcl0rhFNJTGD"
      },
      "outputs": [],
      "source": [
        "#create a scatter plot\n",
        "plt.scatter(matrix[:, 0], matrix[:, 1], c=clusters_customers, s=50, cmap='viridis')\n",
        "#select cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNqA-OpJJTGE"
      },
      "outputs": [],
      "source": [
        "# What's the number of customers in each cluster?\n",
        "pd.DataFrame(pd.Series(clusters_customers).value_counts(), columns = ['NumberCustomers']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aquí se genera un diagrama de dispersión que visualiza la segmentación de clientes en dos clusters, utilizando las primeras dos métricas transformadas de la matriz log_data como ejes. Los puntos de datos están coloreados según su pertenencia a uno de los clusters, con una paleta viridis. Adicionalmente, se marcan los centroides de los clusters en negro para mostrar sus posiciones centrales, con un tamaño mayor y una transparencia ajustada para destacarlos. Finalmente, se calcula la cantidad de clientes en cada cluster mediante un conteo de las asignaciones de clusters (clusters_customers), mostrando los resultados en un DataFrame que indica el número total de clientes por cluster. Esta visualización y conteo facilitan la interpretación del tamaño y distribución de los grupos generados por el modelo K-Means."
      ],
      "metadata": {
        "id": "XYVhzO2Rr5dh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lOBOFfVJTGE"
      },
      "source": [
        "**Note(Additional):** We can check the median of each variable (Frequency, Monetary, Recency) in each cluster in order to understand what customers does each cluster represent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDhYQnUJJTGE"
      },
      "source": [
        "**Conclusion - Perspective after applying k-means clustering**:\n",
        "\n",
        "Unfortunately, we didn't obtain a clearly separated clusters. Clusters assignments are muddled. (It may be due to outliers who weren't removed).\n",
        "\n",
        "Limitations of k-means clustering:\n",
        "- There is no assurance that it will lead to the ***global*** best solution.\n",
        "- Can't deal with **different shapes**(not circular) and consider one point's probability of belonging to more than one cluster.\n",
        "\n",
        "These disadvantages of k-means mean that for many datasets (especially low-dimensional datasets) it may not perform as well as you might hope. Here comes Guassian Mixture Model (GMM) in help by providing greater flexibility due to clusters having unconstrained covariances and allowing probabilistic cluster assignment.\n",
        "\n",
        "Reference  ***Python Data Science Handbook*** by ***Jake VanderPlas***.\n",
        "\n",
        "**Note- Further Explanation:** A common practice before doing clustering: **Principal Component Analysis (PCA)**. PCA calculates the dimensions which best maximize variance.It gives directions on how many components to consider for GMM. Basically, it does **dimensionality reduction** while keeping the most important features, characteristics (combinations of features best describe customers). But as we are not dealing with high dimension we won't do it for this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjWLM0X6JTGE"
      },
      "source": [
        "### Gaussian Mixture Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cTF7qKJTGE"
      },
      "source": [
        "While k-means is easy to understand and implement.It fails when dealing with non-circular shapes and lack of probabilistic cluster assignment—mean that for many datasets (especially low-dimensional datasets) it may not perform as well as you might hope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2gA8ixaJTGE"
      },
      "outputs": [],
      "source": [
        "gmm = GMM(n_components=2).fit(matrix)\n",
        "labels = gmm.predict(matrix)\n",
        "plt.scatter(matrix[:, 0], matrix[:, 1], c=labels, s=40, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aquí seutiliza el modelo Gaussian Mixture Model (GMM) para segmentar los datos en 2 componentes y visualizar los resultados. Primero, el modelo se entrena con los datos transformados (matrix) utilizando GMM(n_components=2), que asume que los datos provienen de una mezcla de dos distribuciones gaussianas. Luego, se predicen las etiquetas de cada punto, asignándolos al componente al que tienen mayor probabilidad de pertenecer. Finalmente, se genera un gráfico de dispersión donde los puntos están coloreados según su cluster, utilizando la paleta viridis. Este método permite una segmentación más flexible que K-Means, ya que considera formas y distribuciones más complejas, proporcionando una visión probabilística de los clusters en los datos."
      ],
      "metadata": {
        "id": "f33zoBC5s4TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANÁLISIS DE LA ACTIVIDAD"
      ],
      "metadata": {
        "id": "TYNolqeDyE4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#El siguiente paso clave es presentar los resultados del clustering a los interesados dentro de la empresa para recoger sus aportaciones y ajustar el análisis en función de sus necesidades estratégicas. Es fundamental comprender cómo la organización planea utilizar la segmentación y qué nivel de detalle desean en la clasificación de los clientes. Por ejemplo, los stakeholders podrían estar interesados en analizar un espectro completo de comportamiento, desde los clientes de mayor valor (que compran frecuentemente, gastan más y lo hacen de manera reciente) hasta los de menor valor (clientes inactivos con baja frecuencia y gasto). La retroalimentación obtenida permitirá refinar las técnicas de clustering, optimizando la segmentación de clientes para que se alinee mejor con los objetivos comerciales. Esto puede implicar trabajar con las métricas RFM o con los datos transaccionales completos para identificar patrones más complejos. Si se dispone de más tiempo, se puede llevar el análisis un paso más allá mediante la implementación de modelos de clasificación para predecir la pertenencia de futuros clientes a los clusters definidos. Esto incluye probar diferentes algoritmos, como Support Vector Classifier (SVC) y Regresión Logística, que son herramientas potentes para clasificar nuevos datos. El proceso implica dividir los datos en conjuntos de entrenamiento y prueba, entrenar cada modelo y luego evaluar su desempeño utilizando métricas como la precisión, la sensibilidad y la puntuación F1. Comparar estos resultados permitirá seleccionar el modelo que ofrezca la mejor capacidad predictiva. Este enfoque no solo ayudará a clasificar correctamente a los nuevos clientes, sino que también permitirá a la empresa anticipar comportamientos futuros y ajustar sus estrategias de marketing, retención o fidelización de manera proactiva y personalizada."
      ],
      "metadata": {
        "id": "IXpCUna7vLLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hemos identificado y segmentado clientes mediante el análisis RFM y técnicas de clustering (K-Means, GMM). Aunque no hemos mencionado explícitamente la tasa de deserción (churn), el análisis de recencia permite inferir clientes con alto riesgo de abandono (por ejemplo, clientes \"Lost\" o \"Almost Lost\"), lo cual forma parte del análisis de churn."
      ],
      "metadata": {
        "id": "7zs6Sh6qycbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aunque se han identificado clientes en riesgo mediante clustering y segmentación RFM, aún faltan recomendaciones específicas para retener a los clientes que están a punto de abandonar o para mejorar la relación con otros segmentos. Para complementar esta pregunta, sería necesario:\n",
        "#Proponer estrategias concretas de retención basadas en los resultados (ej. campañas personalizadas para \"Almost Lost\" o incentivos para \"Loyal Customers\").\n",
        "#Analizar más detalladamente patrones de deserción con datos históricos (ej. identificación de causas específicas de churn).\n",
        "#Evaluar cómo se podrían implementar estas recomendaciones en términos de impacto comercial."
      ],
      "metadata": {
        "id": "OayqbepEyfTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g3_vGuj4vBQt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}